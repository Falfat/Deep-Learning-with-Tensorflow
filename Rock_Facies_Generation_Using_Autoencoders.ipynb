{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Rock_Facies_Generation_Using_Autoencoders.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuYMuLg4gufM"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "from keras import layers"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZV09fWBg3u-",
        "outputId": "a6d7dfea-2308-455c-a96d-ef3a5ff99a2a"
      },
      "source": [
        "ls =[]\n",
        "\n",
        "y = np.random.randint(3, 8, 1)\n",
        "y"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([4])"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOT0haOUnxT3"
      },
      "source": [
        "def randNums(a,b,s):\n",
        "    #finds n random ints in [a,b] with sum of s\n",
        "    import numpy\n",
        "    #select the number of layers randomly\n",
        "    n = np.random.randint(3, 8, 1)\n",
        "    hit = False\n",
        "    while not hit:\n",
        "      #declare number of layers and total thickness\n",
        "        total, count = 0,0\n",
        "      #declare a list to store the thicknesses\n",
        "        nums = []\n",
        "      #if the sum of the thicknesses is less than required\n",
        "      #and number of layers is less tha  required\n",
        "        while total < s and count < n:\n",
        "          #generate a random number of layers\n",
        "            r = np.random.randint(a,b)\n",
        "            #add to total\n",
        "            total += r\n",
        "            #add 1 count\n",
        "            count += 1\n",
        "            #append to list\n",
        "            nums.append(r)\n",
        "        #if either of the 2 conditions are fulfiled, stop.\n",
        "        if total == s and count == n: hit = True\n",
        "    return nums"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7iyDh9jpDRA"
      },
      "source": [
        ""
      ],
      "execution_count": 210,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "se1pyVoCqDmn",
        "outputId": "46466f4f-3f64-468e-aea5-5385f35ae877"
      },
      "source": [
        "ls = []\n",
        "\n",
        "for i in range(10000):\n",
        "  a = randNums(5, 25, 50)\n",
        "  global_main = np.zeros((a[0], 10))\n",
        "  for i in range(global_main.shape[0]):\n",
        "    global_main[i].fill(np.random.lognormal(2.5, 1))\n",
        "  for i in range(1, len(a)):\n",
        "    new = np.zeros((a[i], 10))\n",
        "    for i in range(new.shape[0]):\n",
        "      new[i].fill(np.random.lognormal(2.5, 1))\n",
        "    global_main = np.concatenate((global_main, new), axis=0)\n",
        "  ls.append(global_main)\n",
        "\n",
        "len(ls)\n",
        "data_train = np.array(ls)\n",
        "data_train.shape\n",
        "# for i in range(1, 10):\n",
        "#   a = randNums(5, 25, 50)\n",
        "#   main = np.zeros((a[0], 10))\n",
        "#   for i in range(main.shape[0]):\n",
        "#     main[i].fill(np.random.lognormal(2.5, 1))\n",
        "#   for i in range(1, len(a)):\n",
        "#     new = np.zeros((a[i], 10))\n",
        "#     for i in range(new.shape[0]):\n",
        "#       new[i].fill(np.random.lognormal(2.5, 1))\n",
        "#   global_main = np.vstack((global_main, new))\n"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 50, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYzccaPGqWSA",
        "outputId": "a434a894-48f4-4722-87b1-a5756b5cbe77"
      },
      "source": [
        "ls = []\n",
        "\n",
        "for i in range(1000):\n",
        "  a = randNums(5, 25, 50)\n",
        "  global_main = np.zeros((a[0], 10))\n",
        "  for i in range(global_main.shape[0]):\n",
        "    global_main[i].fill(np.random.lognormal(2.5, 1))\n",
        "  for i in range(1, len(a)):\n",
        "    new = np.zeros((a[i], 10))\n",
        "    for i in range(new.shape[0]):\n",
        "      new[i].fill(np.random.lognormal(2.5, 1))\n",
        "    global_main = np.concatenate((global_main, new), axis=0)\n",
        "  ls.append(global_main)\n",
        "\n",
        "len(ls)\n",
        "data_test = np.array(ls)\n",
        "data_test.shape"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 50, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "gQAJDao8xbU5",
        "outputId": "81316802-32fb-4349-cfa6-f976674e01dc"
      },
      "source": [
        "plt.imshow(data_train[0], cmap='hot', interpolation='nearest')\n",
        "plt.show()"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzvqvedpW3Qj"
      },
      "source": [
        "#flatten data\n",
        "data_train = data_train.reshape((len(data_train), np.prod(data_train.shape[1:])))\n",
        "data_test = data_test.reshape((len(data_test), np.prod(data_test.shape[1:])))"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ibVygMguwis",
        "outputId": "5a5eab97-1a1d-4664-bcd2-fcd9c5e81544"
      },
      "source": [
        "data_train.shape"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 500)"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5bTqzxwKgRp"
      },
      "source": [
        "import pandas as pd\n",
        "pd."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fo5-jJOEu0jv"
      },
      "source": [
        "#construct the model\n",
        "from keras import regularizers\n",
        "\n",
        "#declare a size for the bottleneck\n",
        "encoding_dim = 20 #compression of factor 25, assuming the input is 784 floats (28 by 28 pixels)\n",
        "\n",
        "#input image size\n",
        "input_img = keras.Input(shape=(500,))\n",
        "\n",
        "#encoded representation of the input\n",
        "encoded = layers.Dense(encoding_dim)(input_img)\n",
        "\n",
        "#lossy representation of the input\n",
        "decoded = layers.Dense(500)(encoded)\n",
        "\n",
        "#Main model that maps input to output\n",
        "autoencoder = keras.Model(input_img, decoded)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHtJK7-0KffL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfZf8LwOvwlj"
      },
      "source": [
        "#mapping input to its encoded representation through a separate encoder model\n",
        "encoder = keras.Model(input_img, encoded)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4ZAJq_fwBKO"
      },
      "source": [
        "#creating a separate decoder \n",
        "encoded_input = keras.Input(shape=(encoding_dim,))\n",
        "#retrieve the last layer of the autoencoder model\n",
        "decoded_layer = autoencoder.layers[-1]\n",
        "#create the decoder model\n",
        "decoder = keras.Model(encoded_input, decoded_layer(encoded_input))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3E6FfapOwEEu"
      },
      "source": [
        "#confidure the model to use a per=pixel binary cross entropy loss, and the Adam optimizer\n",
        "autoencoder.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss='binary_crossentropy')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d97qPEvBwGOt",
        "outputId": "0b004354-3846-456c-9e71-8561471567e4"
      },
      "source": [
        "#train autoencoder for 50 epochs\n",
        "from sklearn import preprocessing\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "data_train = min_max_scaler.fit_transform(data_train)\n",
        "data_test = min_max_scaler.fit_transform(data_test)\n",
        "autoencoder.fit(data_train, data_train, epochs=100, batch_size=256, shuffle=True, verbose=1, validation_data=(data_test, data_test))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "40/40 [==============================] - 1s 17ms/step - loss: 0.3526 - val_loss: 0.5963\n",
            "Epoch 2/100\n",
            "40/40 [==============================] - 0s 9ms/step - loss: 0.3180 - val_loss: 0.5545\n",
            "Epoch 3/100\n",
            "40/40 [==============================] - 0s 8ms/step - loss: 0.2936 - val_loss: 0.5212\n",
            "Epoch 4/100\n",
            "40/40 [==============================] - 0s 9ms/step - loss: 0.2739 - val_loss: 0.4916\n",
            "Epoch 5/100\n",
            "40/40 [==============================] - 0s 9ms/step - loss: 0.2571 - val_loss: 0.4650\n",
            "Epoch 6/100\n",
            "40/40 [==============================] - 0s 9ms/step - loss: 0.2426 - val_loss: 0.4413\n",
            "Epoch 7/100\n",
            "40/40 [==============================] - 0s 9ms/step - loss: 0.2307 - val_loss: 0.4219\n",
            "Epoch 8/100\n",
            "40/40 [==============================] - 0s 10ms/step - loss: 0.2209 - val_loss: 0.4041\n",
            "Epoch 9/100\n",
            "40/40 [==============================] - 0s 9ms/step - loss: 0.2124 - val_loss: 0.3886\n",
            "Epoch 10/100\n",
            "40/40 [==============================] - 0s 8ms/step - loss: 0.2047 - val_loss: 0.3738\n",
            "Epoch 11/100\n",
            "40/40 [==============================] - 0s 8ms/step - loss: 0.1983 - val_loss: 0.3592\n",
            "Epoch 12/100\n",
            "40/40 [==============================] - 0s 8ms/step - loss: 0.1925 - val_loss: 0.3471\n",
            "Epoch 13/100\n",
            "40/40 [==============================] - 0s 9ms/step - loss: 0.1875 - val_loss: 0.3367\n",
            "Epoch 14/100\n",
            "40/40 [==============================] - 0s 9ms/step - loss: 0.1833 - val_loss: 0.3282\n",
            "Epoch 15/100\n",
            "40/40 [==============================] - 0s 8ms/step - loss: 0.1799 - val_loss: 0.3206\n",
            "Epoch 16/100\n",
            "40/40 [==============================] - 0s 8ms/step - loss: 0.1771 - val_loss: 0.3135\n",
            "Epoch 17/100\n",
            "40/40 [==============================] - 0s 8ms/step - loss: 0.1748 - val_loss: 0.3073\n",
            "Epoch 18/100\n",
            "40/40 [==============================] - 0s 8ms/step - loss: 0.1731 - val_loss: 0.3020\n",
            "Epoch 19/100\n",
            "40/40 [==============================] - 0s 8ms/step - loss: 0.1716 - val_loss: 0.2982\n",
            "Epoch 20/100\n",
            "40/40 [==============================] - 0s 8ms/step - loss: 0.1702 - val_loss: 0.2945\n",
            "Epoch 21/100\n",
            "40/40 [==============================] - 0s 9ms/step - loss: 0.1688 - val_loss: 0.2904\n",
            "Epoch 22/100\n",
            "40/40 [==============================] - 0s 9ms/step - loss: 0.1677 - val_loss: 0.2874\n",
            "Epoch 23/100\n",
            "40/40 [==============================] - 0s 8ms/step - loss: 0.1668 - val_loss: 0.2848\n",
            "Epoch 24/100\n",
            "40/40 [==============================] - 0s 8ms/step - loss: 0.1660 - val_loss: 0.2820\n",
            "Epoch 25/100\n",
            "40/40 [==============================] - 0s 9ms/step - loss: 0.1652 - val_loss: 0.2801\n",
            "Epoch 26/100\n",
            "40/40 [==============================] - 0s 8ms/step - loss: 0.1644 - val_loss: 0.2779\n",
            "Epoch 27/100\n",
            "40/40 [==============================] - 0s 8ms/step - loss: 0.1635 - val_loss: 0.2757\n",
            "Epoch 28/100\n",
            "40/40 [==============================] - 0s 8ms/step - loss: 0.1628 - val_loss: 0.2739\n",
            "Epoch 29/100\n",
            "40/40 [==============================] - 0s 8ms/step - loss: 0.1621 - val_loss: 0.2717\n",
            "Epoch 30/100\n",
            "40/40 [==============================] - 0s 8ms/step - loss: 0.1617 - val_loss: 0.2702\n",
            "Epoch 31/100\n",
            "40/40 [==============================] - 0s 8ms/step - loss: 0.1613 - val_loss: 0.2686\n",
            "Epoch 32/100\n",
            "40/40 [==============================] - 0s 8ms/step - loss: 0.1610 - val_loss: 0.2674\n",
            "Epoch 33/100\n",
            "40/40 [==============================] - 0s 8ms/step - loss: 0.1607 - val_loss: 0.2666\n",
            "Epoch 34/100\n",
            "40/40 [==============================] - 0s 8ms/step - loss: 0.1604 - val_loss: 0.2659\n",
            "Epoch 35/100\n",
            "40/40 [==============================] - 0s 9ms/step - loss: 0.1602 - val_loss: 0.2650\n",
            "Epoch 36/100\n",
            "40/40 [==============================] - 0s 8ms/step - loss: 0.1600 - val_loss: 0.2644\n",
            "Epoch 37/100\n",
            "40/40 [==============================] - 0s 8ms/step - loss: 0.1598 - val_loss: 0.2636\n",
            "Epoch 38/100\n",
            "40/40 [==============================] - 0s 8ms/step - loss: 0.1597 - val_loss: 0.2629\n",
            "Epoch 39/100\n",
            "40/40 [==============================] - 0s 8ms/step - loss: 0.1595 - val_loss: 0.2622\n",
            "Epoch 40/100\n",
            "40/40 [==============================] - 0s 9ms/step - loss: 0.1593 - val_loss: 0.2619\n",
            "Epoch 41/100\n",
            "40/40 [==============================] - 0s 9ms/step - loss: 0.1592 - val_loss: 0.2613\n",
            "Epoch 42/100\n",
            "40/40 [==============================] - 0s 8ms/step - loss: 0.1591 - val_loss: 0.2607\n",
            "Epoch 43/100\n",
            "40/40 [==============================] - 0s 8ms/step - loss: 0.1589 - val_loss: 0.2603\n",
            "Epoch 44/100\n",
            "40/40 [==============================] - 0s 9ms/step - loss: 0.1588 - val_loss: 0.2600\n",
            "Epoch 45/100\n",
            "40/40 [==============================] - 0s 8ms/step - loss: 0.1587 - val_loss: 0.2598\n",
            "Epoch 46/100\n",
            "40/40 [==============================] - 0s 9ms/step - loss: 0.1586 - val_loss: 0.2592\n",
            "Epoch 47/100\n",
            "40/40 [==============================] - 0s 8ms/step - loss: 0.1585 - val_loss: 0.2586\n",
            "Epoch 48/100\n",
            "40/40 [==============================] - 0s 8ms/step - loss: 0.1584 - val_loss: 0.2583\n",
            "Epoch 49/100\n",
            "40/40 [==============================] - 0s 8ms/step - loss: 0.1583 - val_loss: 0.2580\n",
            "Epoch 50/100\n",
            "40/40 [==============================] - 0s 8ms/step - loss: 0.1582 - val_loss: 0.2577\n",
            "Epoch 51/100\n",
            "40/40 [==============================] - 0s 8ms/step - loss: 0.1581 - val_loss: 0.2575\n",
            "Epoch 52/100\n",
            "40/40 [==============================] - 0s 8ms/step - loss: 0.1580 - val_loss: 0.2572\n",
            "Epoch 53/100\n",
            "40/40 [==============================] - 0s 9ms/step - loss: 0.1579 - val_loss: 0.2570\n",
            "Epoch 54/100\n",
            "40/40 [==============================] - 0s 8ms/step - loss: 0.1578 - val_loss: 0.2569\n",
            "Epoch 55/100\n",
            "40/40 [==============================] - 0s 9ms/step - loss: 0.1577 - val_loss: 0.2566\n",
            "Epoch 56/100\n",
            "40/40 [==============================] - 0s 9ms/step - loss: 0.1577 - val_loss: 0.2564\n",
            "Epoch 57/100\n",
            "40/40 [==============================] - 0s 8ms/step - loss: 0.1576 - val_loss: 0.2561\n",
            "Epoch 58/100\n",
            "40/40 [==============================] - 0s 9ms/step - loss: 0.1575 - val_loss: 0.2559\n",
            "Epoch 59/100\n",
            "40/40 [==============================] - 0s 9ms/step - loss: 0.1575 - val_loss: 0.2553\n",
            "Epoch 60/100\n",
            "40/40 [==============================] - 0s 8ms/step - loss: 0.1574 - val_loss: 0.2551\n",
            "Epoch 61/100\n",
            "40/40 [==============================] - 0s 9ms/step - loss: 0.1573 - val_loss: 0.2549\n",
            "Epoch 62/100\n",
            "40/40 [==============================] - 0s 8ms/step - loss: 0.1572 - val_loss: 0.2549\n",
            "Epoch 63/100\n",
            "40/40 [==============================] - 0s 8ms/step - loss: 0.1571 - val_loss: 0.2546\n",
            "Epoch 64/100\n",
            "40/40 [==============================] - 0s 9ms/step - loss: 0.1571 - val_loss: 0.2545\n",
            "Epoch 65/100\n",
            "40/40 [==============================] - 0s 8ms/step - loss: 0.1570 - val_loss: 0.2544\n",
            "Epoch 66/100\n",
            "40/40 [==============================] - 0s 8ms/step - loss: 0.1569 - val_loss: 0.2542\n",
            "Epoch 67/100\n",
            "40/40 [==============================] - 0s 8ms/step - loss: 0.1569 - val_loss: 0.2541\n",
            "Epoch 68/100\n",
            "40/40 [==============================] - 0s 9ms/step - loss: 0.1568 - val_loss: 0.2539\n",
            "Epoch 69/100\n",
            "40/40 [==============================] - 0s 8ms/step - loss: 0.1568 - val_loss: 0.2537\n",
            "Epoch 70/100\n",
            "40/40 [==============================] - 0s 8ms/step - loss: 0.1567 - val_loss: 0.2535\n",
            "Epoch 71/100\n",
            "40/40 [==============================] - 0s 8ms/step - loss: 0.1566 - val_loss: 0.2534\n",
            "Epoch 72/100\n",
            "40/40 [==============================] - 0s 8ms/step - loss: 0.1566 - val_loss: 0.2534\n",
            "Epoch 73/100\n",
            "40/40 [==============================] - 0s 9ms/step - loss: 0.1565 - val_loss: 0.2533\n",
            "Epoch 74/100\n",
            "40/40 [==============================] - 0s 9ms/step - loss: 0.1565 - val_loss: 0.2532\n",
            "Epoch 75/100\n",
            "40/40 [==============================] - 0s 8ms/step - loss: 0.1564 - val_loss: 0.2532\n",
            "Epoch 76/100\n",
            "40/40 [==============================] - 0s 8ms/step - loss: 0.1564 - val_loss: 0.2531\n",
            "Epoch 77/100\n",
            "40/40 [==============================] - 0s 8ms/step - loss: 0.1563 - val_loss: 0.2529\n",
            "Epoch 78/100\n",
            "40/40 [==============================] - 0s 9ms/step - loss: 0.1563 - val_loss: 0.2526\n",
            "Epoch 79/100\n",
            "40/40 [==============================] - 0s 9ms/step - loss: 0.1562 - val_loss: 0.2524\n",
            "Epoch 80/100\n",
            "40/40 [==============================] - 0s 8ms/step - loss: 0.1562 - val_loss: 0.2523\n",
            "Epoch 81/100\n",
            "40/40 [==============================] - 0s 9ms/step - loss: 0.1561 - val_loss: 0.2522\n",
            "Epoch 82/100\n",
            "40/40 [==============================] - 0s 9ms/step - loss: 0.1561 - val_loss: 0.2521\n",
            "Epoch 83/100\n",
            "40/40 [==============================] - 0s 8ms/step - loss: 0.1560 - val_loss: 0.2521\n",
            "Epoch 84/100\n",
            "40/40 [==============================] - 0s 8ms/step - loss: 0.1560 - val_loss: 0.2519\n",
            "Epoch 85/100\n",
            "40/40 [==============================] - 0s 9ms/step - loss: 0.1559 - val_loss: 0.2518\n",
            "Epoch 86/100\n",
            "40/40 [==============================] - 0s 8ms/step - loss: 0.1559 - val_loss: 0.2517\n",
            "Epoch 87/100\n",
            "40/40 [==============================] - 0s 8ms/step - loss: 0.1559 - val_loss: 0.2517\n",
            "Epoch 88/100\n",
            "40/40 [==============================] - 0s 9ms/step - loss: 0.1558 - val_loss: 0.2516\n",
            "Epoch 89/100\n",
            "40/40 [==============================] - 0s 9ms/step - loss: 0.1558 - val_loss: 0.2515\n",
            "Epoch 90/100\n",
            "40/40 [==============================] - 0s 7ms/step - loss: 0.1557 - val_loss: 0.2513\n",
            "Epoch 91/100\n",
            "40/40 [==============================] - 0s 8ms/step - loss: 0.1557 - val_loss: 0.2512\n",
            "Epoch 92/100\n",
            "40/40 [==============================] - 0s 8ms/step - loss: 0.1557 - val_loss: 0.2511\n",
            "Epoch 93/100\n",
            "40/40 [==============================] - 0s 9ms/step - loss: 0.1556 - val_loss: 0.2510\n",
            "Epoch 94/100\n",
            "40/40 [==============================] - 0s 9ms/step - loss: 0.1556 - val_loss: 0.2508\n",
            "Epoch 95/100\n",
            "40/40 [==============================] - 0s 8ms/step - loss: 0.1555 - val_loss: 0.2507\n",
            "Epoch 96/100\n",
            "40/40 [==============================] - 0s 9ms/step - loss: 0.1555 - val_loss: 0.2506\n",
            "Epoch 97/100\n",
            "40/40 [==============================] - 0s 8ms/step - loss: 0.1555 - val_loss: 0.2506\n",
            "Epoch 98/100\n",
            "40/40 [==============================] - 0s 9ms/step - loss: 0.1554 - val_loss: 0.2505\n",
            "Epoch 99/100\n",
            "40/40 [==============================] - 0s 8ms/step - loss: 0.1554 - val_loss: 0.2502\n",
            "Epoch 100/100\n",
            "40/40 [==============================] - 0s 8ms/step - loss: 0.1554 - val_loss: 0.2501\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f446e1d3c90>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdDvAx6Dwb0M"
      },
      "source": [
        "from sklearn import preprocessing\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "data_train = min_max_scaler.fit_transform(data_train)\n",
        "data_test = min_max_scaler.fit_transform(data_test)\n",
        "input_img = keras.Input(shape=(500,))\n",
        "\n",
        "#encoded and bottleneck layer\n",
        "encoded = layers.Dense(150, activation='relu')(input_img)\n",
        "encoded = layers.Dense(100, activation='relu')(encoded)\n",
        "encoded = layers.Dense(50, activation='relu')(encoded)\n",
        "encoded = layers.Dense(10, activation='relu')(encoded)\n",
        "\n",
        "#decoded and output layers\n",
        "decoded = layers.Dense(50, activation='relu')(encoded)\n",
        "decoded = layers.Dense(100, activation='relu')(decoded)\n",
        "decoded = layers.Dense(150, activation='relu')(decoded)\n",
        "decoded = layers.Dense(500, activation='sigmoid')(decoded)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wokUNukr4AcU",
        "outputId": "910bf15b-40bf-4a8c-8d3a-63520917b6fb"
      },
      "source": [
        "autoencoder = keras.Model(input_img, decoded)\n",
        "autoencoder.compile(optimizer=tf.keras.optimizers.Adam(), loss='binary_crossentropy')\n",
        "\n",
        "autoencoder.fit(data_train, data_train, epochs=100, batch_size=256, shuffle = True, validation_data = (data_test, data_test))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "40/40 [==============================] - 1s 20ms/step - loss: 0.4123 - val_loss: 0.3246\n",
            "Epoch 2/100\n",
            "40/40 [==============================] - 1s 17ms/step - loss: 0.1670 - val_loss: 0.4044\n",
            "Epoch 3/100\n",
            "40/40 [==============================] - 1s 16ms/step - loss: 0.1632 - val_loss: 0.3847\n",
            "Epoch 4/100\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 0.1625 - val_loss: 0.3757\n",
            "Epoch 5/100\n",
            "40/40 [==============================] - 1s 17ms/step - loss: 0.1614 - val_loss: 0.3740\n",
            "Epoch 6/100\n",
            "40/40 [==============================] - 1s 17ms/step - loss: 0.1594 - val_loss: 0.3142\n",
            "Epoch 7/100\n",
            "40/40 [==============================] - 1s 16ms/step - loss: 0.1566 - val_loss: 0.2587\n",
            "Epoch 8/100\n",
            "40/40 [==============================] - 1s 16ms/step - loss: 0.1551 - val_loss: 0.2445\n",
            "Epoch 9/100\n",
            "40/40 [==============================] - 1s 16ms/step - loss: 0.1550 - val_loss: 0.2446\n",
            "Epoch 10/100\n",
            "40/40 [==============================] - 1s 16ms/step - loss: 0.1550 - val_loss: 0.2446\n",
            "Epoch 11/100\n",
            "40/40 [==============================] - 1s 17ms/step - loss: 0.1550 - val_loss: 0.2445\n",
            "Epoch 12/100\n",
            "40/40 [==============================] - 1s 16ms/step - loss: 0.1550 - val_loss: 0.2441\n",
            "Epoch 13/100\n",
            "40/40 [==============================] - 1s 17ms/step - loss: 0.1550 - val_loss: 0.2443\n",
            "Epoch 14/100\n",
            "40/40 [==============================] - 1s 16ms/step - loss: 0.1550 - val_loss: 0.2441\n",
            "Epoch 15/100\n",
            "40/40 [==============================] - 1s 16ms/step - loss: 0.1550 - val_loss: 0.2441\n",
            "Epoch 16/100\n",
            "40/40 [==============================] - 1s 16ms/step - loss: 0.1550 - val_loss: 0.2441\n",
            "Epoch 17/100\n",
            "40/40 [==============================] - 1s 16ms/step - loss: 0.1550 - val_loss: 0.2441\n",
            "Epoch 18/100\n",
            "40/40 [==============================] - 1s 16ms/step - loss: 0.1550 - val_loss: 0.2441\n",
            "Epoch 19/100\n",
            "40/40 [==============================] - 1s 16ms/step - loss: 0.1550 - val_loss: 0.2440\n",
            "Epoch 20/100\n",
            "40/40 [==============================] - 1s 16ms/step - loss: 0.1550 - val_loss: 0.2440\n",
            "Epoch 21/100\n",
            "40/40 [==============================] - 1s 17ms/step - loss: 0.1550 - val_loss: 0.2438\n",
            "Epoch 22/100\n",
            "40/40 [==============================] - 1s 15ms/step - loss: 0.1549 - val_loss: 0.2439\n",
            "Epoch 23/100\n",
            "40/40 [==============================] - 1s 16ms/step - loss: 0.1546 - val_loss: 0.2433\n",
            "Epoch 24/100\n",
            "40/40 [==============================] - 1s 16ms/step - loss: 0.1543 - val_loss: 0.2431\n",
            "Epoch 25/100\n",
            "40/40 [==============================] - 1s 17ms/step - loss: 0.1542 - val_loss: 0.2429\n",
            "Epoch 26/100\n",
            "40/40 [==============================] - 1s 17ms/step - loss: 0.1541 - val_loss: 0.2431\n",
            "Epoch 27/100\n",
            "40/40 [==============================] - 1s 17ms/step - loss: 0.1539 - val_loss: 0.2424\n",
            "Epoch 28/100\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 0.1535 - val_loss: 0.2420\n",
            "Epoch 29/100\n",
            "40/40 [==============================] - 1s 17ms/step - loss: 0.1533 - val_loss: 0.2417\n",
            "Epoch 30/100\n",
            "40/40 [==============================] - 1s 17ms/step - loss: 0.1532 - val_loss: 0.2417\n",
            "Epoch 31/100\n",
            "40/40 [==============================] - 1s 17ms/step - loss: 0.1531 - val_loss: 0.2415\n",
            "Epoch 32/100\n",
            "40/40 [==============================] - 1s 17ms/step - loss: 0.1530 - val_loss: 0.2415\n",
            "Epoch 33/100\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 0.1529 - val_loss: 0.2412\n",
            "Epoch 34/100\n",
            "40/40 [==============================] - 1s 17ms/step - loss: 0.1526 - val_loss: 0.2408\n",
            "Epoch 35/100\n",
            "40/40 [==============================] - 1s 16ms/step - loss: 0.1523 - val_loss: 0.2404\n",
            "Epoch 36/100\n",
            "40/40 [==============================] - 1s 17ms/step - loss: 0.1519 - val_loss: 0.2399\n",
            "Epoch 37/100\n",
            "40/40 [==============================] - 1s 16ms/step - loss: 0.1517 - val_loss: 0.2395\n",
            "Epoch 38/100\n",
            "40/40 [==============================] - 1s 16ms/step - loss: 0.1515 - val_loss: 0.2395\n",
            "Epoch 39/100\n",
            "40/40 [==============================] - 1s 17ms/step - loss: 0.1514 - val_loss: 0.2393\n",
            "Epoch 40/100\n",
            "40/40 [==============================] - 1s 16ms/step - loss: 0.1513 - val_loss: 0.2391\n",
            "Epoch 41/100\n",
            "40/40 [==============================] - 1s 16ms/step - loss: 0.1512 - val_loss: 0.2388\n",
            "Epoch 42/100\n",
            "40/40 [==============================] - 1s 16ms/step - loss: 0.1511 - val_loss: 0.2386\n",
            "Epoch 43/100\n",
            "40/40 [==============================] - 1s 17ms/step - loss: 0.1510 - val_loss: 0.2383\n",
            "Epoch 44/100\n",
            "40/40 [==============================] - 1s 16ms/step - loss: 0.1509 - val_loss: 0.2378\n",
            "Epoch 45/100\n",
            "40/40 [==============================] - 1s 16ms/step - loss: 0.1507 - val_loss: 0.2379\n",
            "Epoch 46/100\n",
            "40/40 [==============================] - 1s 17ms/step - loss: 0.1506 - val_loss: 0.2379\n",
            "Epoch 47/100\n",
            "40/40 [==============================] - 1s 16ms/step - loss: 0.1505 - val_loss: 0.2379\n",
            "Epoch 48/100\n",
            "40/40 [==============================] - 1s 16ms/step - loss: 0.1504 - val_loss: 0.2374\n",
            "Epoch 49/100\n",
            "40/40 [==============================] - 1s 16ms/step - loss: 0.1503 - val_loss: 0.2372\n",
            "Epoch 50/100\n",
            "40/40 [==============================] - 1s 16ms/step - loss: 0.1503 - val_loss: 0.2375\n",
            "Epoch 51/100\n",
            "40/40 [==============================] - 1s 17ms/step - loss: 0.1502 - val_loss: 0.2373\n",
            "Epoch 52/100\n",
            "40/40 [==============================] - 1s 17ms/step - loss: 0.1502 - val_loss: 0.2376\n",
            "Epoch 53/100\n",
            "40/40 [==============================] - 1s 17ms/step - loss: 0.1501 - val_loss: 0.2374\n",
            "Epoch 54/100\n",
            "40/40 [==============================] - 1s 16ms/step - loss: 0.1501 - val_loss: 0.2377\n",
            "Epoch 55/100\n",
            "40/40 [==============================] - 1s 17ms/step - loss: 0.1501 - val_loss: 0.2374\n",
            "Epoch 56/100\n",
            "40/40 [==============================] - 1s 17ms/step - loss: 0.1500 - val_loss: 0.2372\n",
            "Epoch 57/100\n",
            "40/40 [==============================] - 1s 16ms/step - loss: 0.1500 - val_loss: 0.2370\n",
            "Epoch 58/100\n",
            "40/40 [==============================] - 1s 16ms/step - loss: 0.1500 - val_loss: 0.2369\n",
            "Epoch 59/100\n",
            "40/40 [==============================] - 1s 16ms/step - loss: 0.1499 - val_loss: 0.2376\n",
            "Epoch 60/100\n",
            "40/40 [==============================] - 1s 16ms/step - loss: 0.1499 - val_loss: 0.2370\n",
            "Epoch 61/100\n",
            "40/40 [==============================] - 1s 16ms/step - loss: 0.1499 - val_loss: 0.2373\n",
            "Epoch 62/100\n",
            "40/40 [==============================] - 1s 17ms/step - loss: 0.1498 - val_loss: 0.2371\n",
            "Epoch 63/100\n",
            "40/40 [==============================] - 1s 16ms/step - loss: 0.1497 - val_loss: 0.2371\n",
            "Epoch 64/100\n",
            "40/40 [==============================] - 1s 16ms/step - loss: 0.1497 - val_loss: 0.2367\n",
            "Epoch 65/100\n",
            "40/40 [==============================] - 1s 16ms/step - loss: 0.1496 - val_loss: 0.2366\n",
            "Epoch 66/100\n",
            "40/40 [==============================] - 1s 17ms/step - loss: 0.1495 - val_loss: 0.2369\n",
            "Epoch 67/100\n",
            "40/40 [==============================] - 1s 16ms/step - loss: 0.1495 - val_loss: 0.2367\n",
            "Epoch 68/100\n",
            "40/40 [==============================] - 1s 17ms/step - loss: 0.1494 - val_loss: 0.2362\n",
            "Epoch 69/100\n",
            "40/40 [==============================] - 1s 17ms/step - loss: 0.1494 - val_loss: 0.2366\n",
            "Epoch 70/100\n",
            "40/40 [==============================] - 1s 17ms/step - loss: 0.1493 - val_loss: 0.2361\n",
            "Epoch 71/100\n",
            "40/40 [==============================] - 1s 17ms/step - loss: 0.1493 - val_loss: 0.2360\n",
            "Epoch 72/100\n",
            "40/40 [==============================] - 1s 17ms/step - loss: 0.1492 - val_loss: 0.2360\n",
            "Epoch 73/100\n",
            "40/40 [==============================] - 1s 17ms/step - loss: 0.1492 - val_loss: 0.2357\n",
            "Epoch 74/100\n",
            "40/40 [==============================] - 1s 16ms/step - loss: 0.1491 - val_loss: 0.2361\n",
            "Epoch 75/100\n",
            "40/40 [==============================] - 1s 17ms/step - loss: 0.1491 - val_loss: 0.2363\n",
            "Epoch 76/100\n",
            "40/40 [==============================] - 1s 17ms/step - loss: 0.1491 - val_loss: 0.2356\n",
            "Epoch 77/100\n",
            "40/40 [==============================] - 1s 17ms/step - loss: 0.1490 - val_loss: 0.2361\n",
            "Epoch 78/100\n",
            "40/40 [==============================] - 1s 16ms/step - loss: 0.1490 - val_loss: 0.2357\n",
            "Epoch 79/100\n",
            "40/40 [==============================] - 1s 17ms/step - loss: 0.1490 - val_loss: 0.2356\n",
            "Epoch 80/100\n",
            "40/40 [==============================] - 1s 17ms/step - loss: 0.1490 - val_loss: 0.2360\n",
            "Epoch 81/100\n",
            "40/40 [==============================] - 1s 17ms/step - loss: 0.1489 - val_loss: 0.2359\n",
            "Epoch 82/100\n",
            "40/40 [==============================] - 1s 17ms/step - loss: 0.1489 - val_loss: 0.2358\n",
            "Epoch 83/100\n",
            "40/40 [==============================] - 1s 17ms/step - loss: 0.1489 - val_loss: 0.2355\n",
            "Epoch 84/100\n",
            "40/40 [==============================] - 1s 16ms/step - loss: 0.1488 - val_loss: 0.2357\n",
            "Epoch 85/100\n",
            "40/40 [==============================] - 1s 16ms/step - loss: 0.1488 - val_loss: 0.2355\n",
            "Epoch 86/100\n",
            "40/40 [==============================] - 1s 16ms/step - loss: 0.1488 - val_loss: 0.2357\n",
            "Epoch 87/100\n",
            "40/40 [==============================] - 1s 16ms/step - loss: 0.1488 - val_loss: 0.2357\n",
            "Epoch 88/100\n",
            "40/40 [==============================] - 1s 17ms/step - loss: 0.1487 - val_loss: 0.2351\n",
            "Epoch 89/100\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 0.1487 - val_loss: 0.2351\n",
            "Epoch 90/100\n",
            "40/40 [==============================] - 1s 16ms/step - loss: 0.1486 - val_loss: 0.2350\n",
            "Epoch 91/100\n",
            "40/40 [==============================] - 1s 15ms/step - loss: 0.1486 - val_loss: 0.2351\n",
            "Epoch 92/100\n",
            "40/40 [==============================] - 1s 17ms/step - loss: 0.1485 - val_loss: 0.2348\n",
            "Epoch 93/100\n",
            "40/40 [==============================] - 1s 17ms/step - loss: 0.1485 - val_loss: 0.2349\n",
            "Epoch 94/100\n",
            "40/40 [==============================] - 1s 16ms/step - loss: 0.1485 - val_loss: 0.2348\n",
            "Epoch 95/100\n",
            "40/40 [==============================] - 1s 17ms/step - loss: 0.1484 - val_loss: 0.2350\n",
            "Epoch 96/100\n",
            "40/40 [==============================] - 1s 18ms/step - loss: 0.1483 - val_loss: 0.2345\n",
            "Epoch 97/100\n",
            "40/40 [==============================] - 1s 16ms/step - loss: 0.1483 - val_loss: 0.2347\n",
            "Epoch 98/100\n",
            "40/40 [==============================] - 1s 17ms/step - loss: 0.1483 - val_loss: 0.2342\n",
            "Epoch 99/100\n",
            "40/40 [==============================] - 1s 16ms/step - loss: 0.1482 - val_loss: 0.2348\n",
            "Epoch 100/100\n",
            "40/40 [==============================] - 1s 17ms/step - loss: 0.1482 - val_loss: 0.2343\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f446a1afa10>"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lt02rymq53CS"
      },
      "source": [
        "decoded_imgs = autoencoder.predict(data_test)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "NfxziBEh53Fy",
        "outputId": "911172ba-8132-4a33-f387-407f488687ad"
      },
      "source": [
        "n = 10\n",
        "plt.figure(figsize=(20, 4))\n",
        "for i in range(1, n + 1):\n",
        "    # Display original\n",
        "    ax = plt.subplot(2, n, i)\n",
        "    plt.imshow(data_test[i].reshape(50, 10))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # Display reconstruction\n",
        "    ax = plt.subplot(2, n, i + n)\n",
        "    plt.imshow(decoded_imgs[i].reshape(50, 10))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "plt.show()"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x288 with 20 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwW82rIv53Ii"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWW2D2_B53Lq"
      },
      "source": [
        "np.savetxt(\"train.csv\", data_train, delimiter=\",\")"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIoj4itK53OR"
      },
      "source": [
        "np.savetxt(\"test.csv\", data_test, delimiter=\",\")"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1X3b-7Z53Re",
        "outputId": "5f94eb46-93b4-44f0-b0dc-f5c4c28cfcb7"
      },
      "source": [
        "import pandas as pd\n",
        "dat = pd.read_csv(\"train.csv\").values\n",
        "dat.shape"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9999, 500)"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3Eno1yD53UN"
      },
      "source": [
        "input_img = keras.Input(shape = (50, 10, 1))\n",
        "\n",
        "x = layers.Conv2D(16, (3,3), activation = 'relu', padding = 'same')(input_img)\n",
        "x = layers.MaxPooling2D((2, 2), padding = 'same')(x)\n",
        "x = layers.Conv2D(8, (3,3), activation = 'relu', padding = 'same')(x)\n",
        "x = layers.MaxPooling2D((2, 2), padding = 'same')(x)\n",
        "x = layers.Conv2D(8, (3,3), activation = 'relu', padding = 'same')(x)\n",
        "encoded = layers.MaxPooling2D((2, 2), padding = 'same')(x)\n",
        "\n",
        "#Now the representation is (4, 4, 8), i.e 128-dimensional\n",
        "\n",
        "x = layers.Conv2D(8, (3,3), activation = 'relu', padding = 'same')(encoded)\n",
        "x = layers.UpSampling2D((2, 2))(x)\n",
        "x = layers.Conv2D(8, (3,3), activation = 'relu', padding = 'same')(x)\n",
        "x = layers.UpSampling2D((2, 2))(x)\n",
        "x = layers.Conv2D(16, (3,3), activation = 'relu')(x)\n",
        "x = layers.UpSampling2D((2, 2))(x)\n",
        "decoded = layers.Conv2D(1, (3, 3), activation = 'sigmoid', padding = 'same')(x)\n",
        "\n",
        "autoencoder = keras.Model(input_img, decoded)\n",
        "autoencoder.compile(optimizer = 'adam', loss = 'binary_crossentropy')"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzJYkBMJBleg",
        "outputId": "5dbf9ceb-1d06-4379-86b4-acfb7d479c4c"
      },
      "source": [
        "data_test .shape"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 50, 10, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mbTvo8i253XV",
        "outputId": "54dd3269-fc33-4fc0-ed2e-f4607946b341"
      },
      "source": [
        "data_train = np.reshape(data_train, (len(data_train), 50, 10, 1))\n",
        "data_test = np.reshape(data_test, (len(data_test), 50, 10, 1))\n",
        "\n",
        "autoencoder.fit(data_train, data_train,\n",
        "                epochs=50,\n",
        "                batch_size=128,\n",
        "                shuffle=True,\n",
        "                validation_data=(data_test, data_test),\n",
        "                )"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-bb6023791430>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m                 \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                 \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                 \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    922\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3036\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3037\u001b[0m       (graph_function,\n\u001b[0;32m-> 3038\u001b[0;31m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0m\u001b[1;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3458\u001b[0m               call_context_key in self._function_cache.missed):\n\u001b[1;32m   3459\u001b[0m             return self._define_function_with_shape_relaxation(\n\u001b[0;32m-> 3460\u001b[0;31m                 args, kwargs, flat_args, filtered_flat_args, cache_key_context)\n\u001b[0m\u001b[1;32m   3461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3462\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_define_function_with_shape_relaxation\u001b[0;34m(self, args, kwargs, flat_args, filtered_flat_args, cache_key_context)\u001b[0m\n\u001b[1;32m   3380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3381\u001b[0m     graph_function = self._create_graph_function(\n\u001b[0;32m-> 3382\u001b[0;31m         args, kwargs, override_flat_arg_shapes=relaxed_arg_shapes)\n\u001b[0m\u001b[1;32m   3383\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marg_relaxed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrank_only_cache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3306\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3307\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3308\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3309\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3310\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1005\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    992\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 994\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    995\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:853 train_function  *\n        return step_function(self, iterator)\n    /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:842 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:1286 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:2849 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:3632 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:835 run_step  **\n        outputs = model.train_step(data)\n    /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:789 train_step\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    /usr/local/lib/python3.7/dist-packages/keras/engine/compile_utils.py:201 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    /usr/local/lib/python3.7/dist-packages/keras/losses.py:141 __call__\n        losses = call_fn(y_true, y_pred)\n    /usr/local/lib/python3.7/dist-packages/keras/losses.py:245 call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    /usr/local/lib/python3.7/dist-packages/keras/losses.py:1809 binary_crossentropy\n        backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    /usr/local/lib/python3.7/dist-packages/keras/backend.py:5000 binary_crossentropy\n        return tf.nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/nn_impl.py:246 sigmoid_cross_entropy_with_logits_v2\n        logits=logits, labels=labels, name=name)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/nn_impl.py:133 sigmoid_cross_entropy_with_logits\n        (logits.get_shape(), labels.get_shape()))\n\n    ValueError: logits and labels must have the same shape ((None, 52, 12, 1) vs (None, 50, 10, 1))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elcBa0CG53aC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMljSQ4L53c5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}